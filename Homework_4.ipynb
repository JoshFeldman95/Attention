{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OSMDXPy8M7C"
   },
   "source": [
    "# HW 4 - All About Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz5Kh9F0xBVf"
   },
   "source": [
    "Welcome to CS 287 HW4. To begin this assignment first turn on the Python 3 and GPU backend for this Colab by clicking `Runtime > Change Runtime Type` above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiugnUMt8M7E"
   },
   "source": [
    "In this homework you will be reproducing the decomposable attention model in Parikh et al. https://aclweb.org/anthology/D16-1244. (This is one of the models that inspired development of the transformer). \n",
    "\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you finish the following goals in PyTorch:\n",
    "\n",
    "1. Implement the vanilla decomposable attention model as described in that paper.\n",
    "2. Implement the decomposable attention model with intra attention or another extension.\n",
    "3. Visualize the attentions in the above two parts.\n",
    "4. Implement a mixture of models with uniform prior and perform training with exact log marginal likelihood (see below for detailed instructions)\n",
    "5. Train the mixture of models in part 4 with VAE. (This may not produce a better model, this is still a research area) \n",
    "6. Interpret which component specializes at which type of tasks using the posterior.\n",
    "\n",
    "Consult the paper for model architecture and hyperparameters, but you are also allowed to tune the hyperparameters yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load\n",
    "from models import AttentionModel\n",
    "\n",
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, TEXT, LABEL = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch', 16), ('logit', 4)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "model = AttentionModel(TEXT, LABEL, 100, 100, intra_attn = True)\n",
    "model(batch.premise, batch.hypothesis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentVariableMixtureModel(ntorch.nn.Module):\n",
    "    def __init__(self, model, experts, variational):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            self.models = []\n",
    "            for _ in range(experts):\n",
    "                self.models.append(copy.deepcopy(model))\n",
    "        except RuntimeError:\n",
    "            raise RuntimeError(\"model must be newly instantiated\")\n",
    "        self.experts = experts\n",
    "        self.variational = variational\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        if self.variational:\n",
    "            return self.sample(premise, hypothesis)\n",
    "        else:\n",
    "            return self.enumerate(premise, hypothesis)\n",
    "    \n",
    "    def self.sample(premise, hypothesis):\n",
    "        pass\n",
    "    \n",
    "    def enumerate(self, premise, hypothesis):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            predictions.append(model(premise, hypothesis))\n",
    "        return (\n",
    "            ntorch.stack(predictions, \"experts\")\n",
    "            .softmax('logit')\n",
    "            .mean('experts')\n",
    "            .log()\n",
    "            .rename('logit','logprob')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(TEXT, LABEL, 100, 100, intra_attn = True)\n",
    "mixture_model = LatentVariableMixtureModel(model, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWjR3JkUk1_A"
   },
   "source": [
    "### Instructions for latent variable mixture model.\n",
    "\n",
    "For the last part of this assignment we will consider a latent variable version of this model. This is a use of latent variable as a form of ensembling.\n",
    "\n",
    "Instead of a single model, we use $K$ models $p(y | \\mathbf{a}, \\mathbf{b}; \\theta_k)$ ($k=1,\\cdots,K$), where $K$ is a hyperparameter. Let's introduce a discrete latent variable $c\\sim \\text{Uniform}(1,\\cdots, K)$ denoting which model is being used to produce the label $y$, then the marginal likelihood is\n",
    "\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{a}, \\mathbf{b}; \\theta) = \\sum_{c=1}^K p(c) p(y | \\mathbf{a}, \\mathbf{b}; \\theta_c)\n",
    "$$\n",
    "\n",
    "When $K$ is small, we can *enumerate* all possible values of $c$ to maximize the log marginal likelihood. \n",
    "\n",
    "We can also use variational auto encoding to perform efficient training. We first introduce an inference network $q(c| y, \\mathbf{a}, \\mathbf{b})$, and the ELBO is\n",
    "\n",
    "$$\n",
    "\\log p(y|\\mathbf{a}, \\mathbf{b}; \\theta)  \\ge \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) - KL(q(c|y, \\mathbf{a}, \\mathbf{b})|| p(c)),\n",
    "$$\n",
    "\n",
    "where $p(c)$ is the prior uniform distribution. We can calculate the $KL$ term in closed form, but for the first term in ELBO, due to the discreteness of $c$, we cannot use the reparameterization trick. Instead we use REINFORCE to estimate the gradients (or see slides):\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) = \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\left [\\nabla \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) + \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c)  \\nabla \\log q(c|y, \\mathbf{a}, \\mathbf{b})\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "At inference time, to get $p(y|\\mathbf{a}, \\mathbf{b}; \\theta)$ we use enumeration to calculate it exactly. For posterior inference, we can either use $q(c| y, \\mathbf{a}, \\mathbf{b})$ to approximate the true posterior or use Bayes rule to calculate the posterior exactly.\n",
    "\n",
    "To interpret what specialized knowledge each component $c$ learns, we can find those examples whose posterior reaches maximum at $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ptva0JXkxcF"
   },
   "source": [
    "When a model is trained, use the following test function to produce predictions, and then upload your best result to the kaggle competition:  https://www.kaggle.com/c/harvard-cs287-s19-hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw_PRRx18M72"
   },
   "outputs": [],
   "source": [
    "def test_code(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "    test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = model(batch.text)\n",
    "        # here we assume that the name for dimension classes is `classes`\n",
    "        _, argmax = probs.max('classes')\n",
    "        upload += argmax.tolist()\n",
    "\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        for u in upload:\n",
    "            f.write(str(u) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiFYDx_58M76"
   },
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework 4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
